# Investigating-LLM-s-Hallucinations-on-Known-Facts

• Analyzed internal inference dynamics of LLMs (LLaMA3-8B, GPT-2, Pythia-70M, OPT-125M) to study
why hallucinations occur despite correct knowledge being stored.

• Developed SVM-based classifiers leveraging these dynamics, achieving 92% accuracy in detecting hallucinations
across models.
